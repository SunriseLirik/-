1. **Возможные гиперпараметры модели линейной регрессии:**

- **Смещение (bias):** Используется для корректировки линии регрессии.
- **Скорость обучения (learning rate):** Определяет размер шага на каждой итерации при поиске минимума функции потерь.
- **Регуляризация:**
  - **L1 Regularization (Lasso):** Добавляет штраф, пропорциональный абсолютному значению коэффициентов.
  - **L2 Regularization (Ridge):** Добавляет штраф, пропорциональный квадрату коэффициентов.
  - **Elastic Net:** Комбинация L1 и L2 регуляризации.
  - **Коэффициент регуляризации (alpha):** Степень влияния регуляризации.
- **Термин рандомизации (random_state):** Используется для воспроизводимости процесса обучения.
- **Количество итераций (max_iter):** Определяет максимальное количество итераций, которые модель будет проходить.

2. **Может ли коэффициент детерминации (R²) быть отрицательным числом?**

Да, коэффициент детерминации может быть отрицательным. Он представляет собой долю дисперсии зависимой переменной, объясняемую независимыми переменными. Если модель хуже, чем простое предсказание среднего значения зависимой переменной, R² будет меньше нуля.

3. **Оценка MSE (Mean Squared Error) для данных:**

Реальные значения y: {1, 2, 3, 4}
Предсказанные значения: {2, 1, 4, 6}

MSE = (1/4) * [(1-2)² + (2-1)² + (3-4)² + (4-6)²]
     = (1/4) * [1 + 1 + 1 + 4]
     = (1/4) * 7
     = 1.75

4. **Рассчитайте обновленный вектор весов:**

Вектор весов w: {10, 5, 6}
Градиент функции потерь: {20, -10, 40}
Скорость обучения: 0.1

Обновленный вектор весов w_new = w - learning_rate * gradient
                             = {10, 5, 6} - 0.1 * {20, -10, 40}
                             = {10, 5, 6} - {2, -1, 4}
                             = {8, 6, 2}

5. **Необходимые данные для расчета градиента функции потерь:**

- **Объективная функция (функция потерь):** Определяет, как оценивается ошибка модели.
- **Вектор весов модели:** Текущие значения весов модели.
- **Примеры данных:** Входные данные и соответствующие им реальные выходные данные.
- **Параметры модели:** Количество признаков и другие параметры, которые могут влиять на расчет градиента (например, регуляризация).

6. **Определение векторов весов с регуляризацией:**

С учетом предоставленной информации, вектор весов с регуляризацией (w2 {0.69, 2.02, 4.20}) вероятнее всего представляет уменьшенные коэффициенты по сравнению с w1 {14.37, 22.80, 32.20}. Это связано с тем, что регуляризация обычно склонна уменьшать абсолютное значение весов модели, чтобы предотвратить переобучение.

7. **Оценка предсказания модели для x {1, 3, 1}:**

Весы модели w: {3, -2, 2}
Параметры x: {1, 3, 1}

Предсказанное значение: w * x
                    = {3, -2, 2} * {1, 3, 1}
                    = (3*1) + (-2*3) + (2*1)
                    = 3 - 6 + 2
                    = -1

8. **Рассчитайте коэффициент детерминации:**

Реальные значения y: {1, 2, 3, 4}
Предсказанные значения: {2, 1, 4, 6}

Сначала вычислите среднее реальных значений:
mean_y = (1 + 2 + 3 + 4) / 4 = 2.5

Затем вычислите сумму квадратов и дисперсию реальных значений:
SS_tot = (1-2.5)² + (2-2.5)² + (3-2.5)² + (4-2.5)² = 5

Вычислите сумму квадратов остатков (погрешность):
SS_res = (1-2)² + (2-1)² + (3-4)² + (4-6)² = 10

Теперь вычислите R²:
R² = 1 - (SS_res / SS_tot)
    = 1 - (10 / 5)
    = 1 - 2
    = -1

Однако R² не может быть отрицательным, что указывает на ошибку в расчете. Давайте исправим его:

SS_res = (1-2)² + (2-1)² + (3-4)² + (4-6)² = 10
SS_tot = 5 (как было рассчитано ранее)

R²
