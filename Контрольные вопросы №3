1. Возможные гиперпараметры модели линейной регрессии:

- Смещение (bias): Используется для корректировки линии регрессии.
- Скорость обучения (learning rate): Определяет размер шага на каждой итерации при поиске минимума функции потерь.
- Регуляризация:
  - L1 Regularization (Lasso): Добавляет штраф, пропорциональный абсолютному значению коэффициентов.
  - L2 Regularization (Ridge): Добавляет штраф, пропорциональный квадрату коэффициентов.
  - Elastic Net: Комбинация L1 и L2 регуляризации.
  - Коэффициент регуляризации (alpha): Степень влияния регуляризации.
- Термин рандомизации (random_state): Используется для воспроизводимости процесса обучения.
- Количество итераций (max_iter): Определяет максимальное количество итераций, которые модель будет проходить.

2. Может ли коэффициент детерминации (R²) быть отрицательным числом?

Да, коэффициент детерминации может быть отрицательным. Он представляет собой долю дисперсии зависимой переменной, объясняемую независимыми переменными. Если модель хуже, чем простое предсказание среднего значения зависимой переменной, R² будет меньше нуля.

3. Оценка MSE (Mean Squared Error) для данных:

Реальные значения y: {1, 2, 3, 4}
Предсказанные значения: {2, 1, 4, 6}

MSE = (1/4) * [(1-2)² + (2-1)² + (3-4)² + (4-6)²]
     = (1/4) * [1 + 1 + 1 + 4]
     = (1/4) * 7
     = 1.75

Для вычисления среднеквадратической ошибки (MSE, Mean Squared Error) в Python, можно использовать библиотеку numpy, которая облегчает работу с массивами и математические операции. MSE является популярной метрикой для оценки точности числовых предсказаний, и она рассчитывается как среднее квадратов разностей между истинными и предсказанными значениями.

Формула MSE:

[ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 ]

где:

( y_i ) — истинное значение,
( \hat{y}_i ) — предсказанное значение,
( n ) — количество элементов в данных.

import numpy as np

def mean_squared_error(y_true, y_pred):
    """Вычисление среднеквадратической ошибки (MSE)"""
    # Преобразуем списки в массивы numpy для удобства вычислений
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    # Вычисляем квадраты разностей
    squared_errors = (y_true - y_pred) ** 2
    # Среднее значение квадратов разностей
    mse = squared_errors.mean()
    return mse

# Истинные значения
y_true = [1, 2, 3, 4]
# Предсказанные значения
y_pred = [2, 1, 4, 6]

# Вычисление MSE
mse_result = mean_squared_error(y_true, y_pred)

print(f"MSE для заданных данных: {mse_result:.2f}")
Объяснение кода:
-Функция mean_squared_error принимает два списка: y_true (истинные значения) и y_pred (предсказанные значения). Эти списки преобразуются в массивы NumPy для удобства вычислений.
-Вычисляется квадрат разностей между соответствующими элементами этих массивов.
-Среднее значение этих квадратов разностей (MSE) рассчитывается с использованием метода .mean() массива NumPy.
-Наконец, результат выводится с двумя знаками после запятой для чистоты отображения.

4. Рассчитайте обновленный вектор весов:

Вектор весов w: {10, 5, 6}
Градиент функции потерь: {20, -10, 40}
Скорость обучения: 0.1

Обновленный вектор весов w_new = w - learning_rate * gradient
                             = {10, 5, 6} - 0.1 * {20, -10, 40}
                             = {10, 5, 6} - {2, -1, 4}
                             = {8, 6, 2}

Для обновления вектора весов в контексте градиентного спуска, мы можем использовать Python для выполнения необходимых вычислений. Вектор весов обновляется путем вычитания произведения скорости обучения и градиента по этим весам. Это общий подход в алгоритмах машинного обучения, особенно в обучении нейронных сетей.

Формула обновления весов:
[ w_{\text{new}} = w - \text{learning_rate} \times \text{gradient} ]

Где:
( w ) — текущий вектор весов,
( \text{gradient} ) — градиент функции потерь по весам,
( \text{learning_rate} ) — скорость обучения (шаг обучения).

import numpy as np

def update_weights(w, gradient, learning_rate):
    """Обновление вектора весов по правилу градиентного спуска."""
    # Преобразование списков в массивы NumPy для удобства вычислений
    w = np.array(w)
    gradient = np.array(gradient)
    # Вычисление обновленного вектора весов
    w_new = w - learning_rate * gradient
    return w_new

# Исходный вектор весов
w = [10, 5, 6]
# Градиент функции потерь
gradient = [20, -10, 40]
# Скорость обучения
learning_rate = 0.1

# Вычисление нового вектора весов
w_new = update_weights(w, gradient, learning_rate)

print("Обновленный вектор весов:", w_new)
Объяснение кода:
-Функция update_weights принимает три аргумента: текущий вектор весов w, градиент gradient и скорость обучения learning_rate.
-Внутри функции, векторы w и gradient преобразуются в массивы NumPy, что позволяет выполнить векторизованные операции над ними.
-Вычисление нового вектора весов производится путем вычитания произведения скорости обучения и градиента из текущего вектора весов.
-Результат представляет собой массив NumPy, который затем выводится на экран.

5. Необходимые данные для расчета градиента функции потерь:

- Объективная функция (функция потерь): Определяет, как оценивается ошибка модели.
- Вектор весов модели: Текущие значения весов модели.
- Примеры данных: Входные данные и соответствующие им реальные выходные данные.
- Параметры модели: Количество признаков и другие параметры, которые могут влиять на расчет градиента (например, регуляризация).

6. Определение векторов весов с регуляризацией:

С учетом предоставленной информации, вектор весов с регуляризацией (w2 {0.69, 2.02, 4.20}) вероятнее всего представляет уменьшенные коэффициенты по сравнению с w1 {14.37, 22.80, 32.20}. Это связано с тем, что регуляризация обычно склонна уменьшать абсолютное значение весов модели, чтобы предотвратить переобучение.

7. Оценка предсказания модели для x {1, 3, 1}:

Весы модели w: {3, -2, 2}
Параметры x: {1, 3, 1}

Предсказанное значение: w * x
                    = {3, -2, 2} * {1, 3, 1}
                    = (3*1) + (-2*3) + (2*1)
                    = 3 - 6 + 2
                    = -1

Для вычисления предсказанного значения модели, где модель представлена в виде линейной комбинации весов и параметров входных данных, мы можем использовать Python. Это типичная задача в машинном обучении, где модель оценивает выходное значение на основе входных признаков и набора весов.

Формула предсказания:
[ \text{Предсказанное значение} = w \cdot x ]
где ( w ) — вектор весов модели, а ( x ) — вектор входных параметров.

def predict(w, x):
    """Расчет предсказанного значения модели для данного входа x."""
    # Проверяем, что векторы одинаковой длины
    if len(w) != len(x):
        raise ValueError("Векторы весов и входных параметров должны быть одинаковой длины")
    
    # Вычисляем скалярное произведение векторов
    predicted_value = sum(weight * param for weight, param in zip(w, x))
    return predicted_value

# Весы модели
w = [3, -2, 2]
# Параметры входных данных
x = [1, 3, 1]

# Вычисление предсказанного значения
predicted_value = predict(w, x)

print("Предсказанное значение модели:", predicted_value)
Объяснение кода:
-Функция predict принимает два аргумента: w (вектор весов) и x (вектор параметров).
-Внутри функции, сначала проверяется, что длины векторов w и x совпадают. Это важно для корректного вычисления скалярного произведения.
-Затем производится расчет скалярного произведения векторов. Это делается с помощью генератора, который перемножает соответствующие элементы векторов и суммирует результаты.
-Результат представляет собой предсказанное значение, которое затем выводится на экран.

8. Рассчитайте коэффициент детерминации:

Реальные значения y: {1, 2, 3, 4}
Предсказанные значения: {2, 1, 4, 6}

Сначала вычислите среднее реальных значений:
mean_y = (1 + 2 + 3 + 4) / 4 = 2.5

Затем вычислите сумму квадратов и дисперсию реальных значений:
SS_tot = (1-2.5)² + (2-2.5)² + (3-2.5)² + (4-2.5)² = 5

Вычислите сумму квадратов остатков (погрешность):
SS_res = (1-2)² + (2-1)² + (3-4)² + (4-6)² = 10

Теперь вычислите R²:
R² = 1 - (SS_res / SS_tot)
    = 1 - (10 / 5)
    = 1 - 2
    = -1

Однако R² не может быть отрицательным, что указывает на ошибку в расчете. Давайте исправим его:

SS_res = (1-2)² + (2-1)² + (3-4)² + (4-6)² = 10
SS_tot = 5 (как было рассчитано ранее)

R²
Python:

def calculate_r_squared(y_true, y_pred):
    # Среднее реальных значений
    mean_y = sum(y_true) / len(y_true)
    
    # Сумма квадратов отклонений (total sum of squares)
    ss_tot = sum((y - mean_y) ** 2 for y in y_true)
    
    # Сумма квадратов ошибок (residual sum of squares)
    ss_res = sum((y_true_i - y_pred_i) ** 2 for y_true_i, y_pred_i in zip(y_true, y_pred))
    
    # Коэффициент детерминации R^2
    r_squared = 1 - (ss_res / ss_tot)
    
    return r_squared

# Реальные значения
y_true = [1, 2, 3, 4]

# Предсказанные значения
y_pred = [2, 1, 4, 6]

# Расчёт R^2
r_squared = calculate_r_squared(y_true, y_pred)

print(f"Коэффициент детерминации R² = {r_squared}")
Объяснение кода:
-Функция calculate_r_squared принимает два списка: y_true (реальные значения) и y_pred (предсказанные значения).
-Сначала рассчитывается среднее реальных значений (mean_y).
-Затем вычисляется общая сумма квадратов отклонений от среднего (ss_tot) и сумма квадратов ошибок (ss_res).
-На основе этих сумм вычисляется ( R^2 ), который показывает, насколько хорошо предсказания соответствуют реальным данным. Если ( R^2 ) отрицательный, это указывает на то, что модель хуже, чем простое предсказание средним значением.
