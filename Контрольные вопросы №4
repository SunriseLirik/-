1. Ошибка теста: ложно-положительный результат (False Positive, FP).
   Это означает, что тест показал наличие заболевания, хотя на самом деле человек здоров.

2. Метрики классификации для матрицы ошибок TP = 5, TN = 90, FP = 10, FN = 5:
   - Точность (Accuracy): (TP + TN) / (TP + TN + FP + FN) = (5 + 90) / 100 = 0.95
   - Чувствительность (Sensitivity, Recall): TP / (TP + FN) = 5 / (5 + 5) = 0.5
   - Специфичность (Specificity): TN / (TN + FP) = 90 / (90 + 10) = 0.9
   - Полнота (Precision): TP / (TP + FP) = 5 / (5 + 10) = 0.33
   - F1-мера: 2 * (Precision * Recall) / (Precision + Recall) = 2 * (0.33 * 0.5) / (0.33 + 0.5) ≈ 0.38

3. Сравнение классификаторов:
   - Первый классификатор имеет очень высокую чувствительность, но низкую специфичность, что может указывать на много ложно-положительных результатов.
   - Второй классификатор имеет более сбалансированные показатели чувствительности и специфичности, что делает его более надежным для обнаружения обоих классов.
   Данные для классификации, вероятно, имеют различную структуру и распределение для каждого классификатора.

4. Гиперпараметры модели логистической регрессии:
   - Регуляризация (например, L1 и L2, параметры λ или C)
   - Метод оптимизации и его параметры (например, скорость обучения в градиентном спуске)
   - Число итераций для обучения модели
   - Функция потерь

5. Чтобы проанализировать веса моделей при использовании только числовых признаков и определить параметр, который в наибольшей степени связан с целевой переменной, можно воспользоваться линейной регрессией. Для этого можно использовать библиотеку scikit-learn. Предположим, что целевая переменная — это столбец 'Price(euro)'. Вот шаги, которые нужно выполнить:
-Загрузите данные в DataFrame.
-Выберите только числовые признаки.
-Разделите данные на обучающие и тестовые наборы.
-Обучите модель линейной регрессии.
-Проанализируйте веса модели, чтобы определить, какой параметр наиболее связан с целевой переменной.
Пример кода:

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import numpy as np
# Шаг 1: Загрузка данных в DataFrame (предположим, что данные в файле 'cars.csv')
df = pd.read_csv('cars.csv')
# Шаг 2: Выбор только числовых признаков и целевой переменной
numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()
numerical_features.remove('Price(euro)')  # Удаляем целевую переменную из списка признаков
X = df[numerical_features]
y = df['Price(euro)']
# Шаг 3: Разделение данных на обучающие и тестовые наборы
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Шаг 4: Обучение модели линейной регрессии
model = LinearRegression()
model.fit(X_train, y_train)
# Шаг 5: Анализ весов модели
coefficients = pd.Series(model.coef_, index=numerical_features)
coefficients = coefficients.sort_values(ascending=False)
print("Веса модели линейной регрессии:")
print(coefficients)
# Параметр, который в наибольшей степени связан с целевой переменной
most_related_feature = coefficients.idxmax()
print(f'Параметр, который в наибольшей степени связан с ценой: {most_related_feature}')
Описание шагов:
-Импортируйте необходимые библиотеки: pandas для работы с данными, scikit-learn для машинного обучения, и numpy для работы с числовыми данными.
-Загрузите данные из файла 'cars.csv' в DataFrame.
-Выберите только числовые признаки, исключив целевую переменную ('Price(euro)').
-Разделите данные на обучающие и тестовые наборы с помощью функции train_test_split.
-Обучите модель линейной регрессии на обучающих данных.
-Проанализируйте веса модели, которые можно получить с помощью атрибута coef_ модели линейной регрессии. Отсортируйте веса, чтобы определить, какой параметр имеет наибольшее значение.

6. Значение функции сигмоиды σ(z) для z = 0.25:
   σ(0.25) = 1 / (1 + e^(-0.25)) ≈ 0.5596
Для вычисления значения функции сигмоиды σ(z) при z = 0.25 на Python, мы можем использовать библиотеку math для доступа к функции экспоненты. Функция сигмоиды определяется формулой:

[ \sigma(z) = \frac{1}{1 + e^{-z}} ]

где ( e ) — основание натурального логарифма.

import math

def sigmoid(z):
    """Вычисление значения сигмоидной функции"""
    return 1 / (1 + math.exp(-z))

# Значение z
z = 0.25

# Вычисление сигмоиды для z
sigmoid_value = sigmoid(z)

print(f"Значение функции сигмоиды σ({z}) = {sigmoid_value:.4f}")
В этом коде:
-Функция sigmoid принимает в качестве аргумента значение ( z ) и возвращает значение сигмоидной функции для этого ( z ).
-Мы используем math.exp для вычисления ( e^{-z} ).
-Форматирование вывода .4f используется для ограничения вывода до четырёх знаков после запятой, что позволяет нам получить приблизительное значение 0.5596.

7. Значение производной функции сигмоиды σ'(z) для z = -3:
   σ'(-3) = σ(-3) * (1 - σ(-3)) = (1 / (1 + e^3)) * (e^3 / (1 + e^3)) ≈ 0.0474
Для вычисления значения производной функции сигмоиды ( \sigma'(z) ) при ( z = -3 ) в Python, мы снова воспользуемся библиотекой math. Производная функции сигмоиды по ( z ) выражается через саму функцию сигмоиды следующим образом:

[ \sigma'(z) = \sigma(z) \cdot (1 - \sigma(z)) ]

Это уравнение следует из того, что сигмоидная функция имеет приятное свойство, когда ее производная может быть выражена через саму функцию, что делает вычисления более прямолинейными.

import math

def sigmoid(z):
    """Вычисление значения сигмоидной функции"""
    return 1 / (1 + math.exp(-z))

def sigmoid_derivative(z):
    """Вычисление значения производной сигмоидной функции"""
    sig = sigmoid(z)
    return sig * (1 - sig)

# Значение z
z = -3

# Вычисление производной сигмоиды для z
sigmoid_derivative_value = sigmoid_derivative(z)

print(f"Значение производной функции сигмоиды σ'({z}) = {sigmoid_derivative_value:.4f}")
В этом примере:
-Функция sigmoid вычисляет значение сигмоидной функции.
-Функция sigmoid_derivative использует значение sigmoid(z) для вычисления производной сигмоиды по формуле ( \sigma'(z) = \sigma(z) \cdot (1 - \sigma(z)) ).
-Значение ( z = -3 ) передается в функцию для вычисления производной.
Результат выводится с помощью print, форматируя вывод до четырех знаков после запятой для точности.

8. Классификация по логистической модели для z = 0.1 с порогом 0.6:
   Поскольку σ(0.1) ≈ 0.5244, что меньше порога 0.6, результат классификации будет отнесен к отрицательному классу.
Для выполнения классификации по логистической модели на Python, мы сначала определим функцию сигмоиды, которая будет вычислять вероятность принадлежности к положительному классу. Затем мы создадим функцию для классификации, которая принимает значение ( z ) и порог (threshold), и на основании сравнения значения сигмоиды с порогом определяет класс объекта.

import math

def sigmoid(z):
    """Вычисление значения сигмоидной функции"""
    return 1 / (1 + math.exp(-z))

def classify(z, threshold=0.6):
    """Классификация с использованием порога для сигмоидной функции"""
    probability = sigmoid(z)
    if probability >= threshold:
        return 'положительный класс'
    else:
        return 'отрицательный класс'

# Значение z
z = 0.1

# Классификация для заданного z с порогом 0.6
classification_result = classify(z)

print(f"Классификация для z = {z} с порогом 0.6: {classification_result}")
Объяснение кода:
-Функция sigmoid(z): Рассчитывает вероятность принадлежности к положительному классу для данного значения ( z ).
-Функция classify(z, threshold=0.6): Принимает значение ( z ) и порог (по умолчанию 0.6). Функция сравнивает значение сигмоиды с порогом и возвращает строку, описывающую классификацию объекта.
-Вызов функции classify(z): Передаем значение ( z = 0.1 ) и получаем результат классификации. Если значение сигмоиды больше или равно 0.6, объект классифицируется как принадлежащий к положительному классу; если меньше — к отрицательному.

9. Значение бинарной кросс-энтропии для предсказания y' = 0.1 и целевой переменной y = 1, где y' - это предсказание модели y:*
   L(y, y') = -y * log(y') - (1 - y) * log(1 - y') = -1 * log(0.1) - (1 - 1) * log(0.9) ≈ 2.3026

Решение на Python:

import numpy as np
from sklearn.metrics import log_loss
# Заданные значения
y_true = 1
y_pred = 0.1
# Вычисление вручную
bce_manual = - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
print(f'Бинарная кросс-энтропия (вычисление вручную): {bce_manual}')

# Проверка с использованием scikit-learn
# log_loss принимает массивы, поэтому преобразуем значения в массивы
bce_sklearn = log_loss([y_true], [y_pred], labels=[0, 1])
print(f'Бинарная кросс-энтропия (scikit-learn): {bce_sklearn}')
Объяснение кода:
Импортируем необходимые библиотеки: numpy для математических операций и log_loss из sklearn.metrics для вычисления логарифмической потери.
Задаем истинное значение y_true и предсказанное значение y_pred.
Вычисляем бинарную кросс-энтропию вручную, используя формулу.
Для проверки используем функцию log_loss из библиотеки scikit-learn.
Запуск кода должен вывести следующее:
Бинарная кросс-энтропия (вычисление вручную): 2.3025850929940455
Бинарная кросс-энтропия (scikit-learn): 2.3025850929940455
Оба метода должны дать одно и то же значение, что подтверждает правильность вычислений.
